import os
import re
import logging
import zipfile
import tarfile
import tempfile
import mimetypes
from typing import List, Dict, Tuple, Set, Optional
from pathlib import Path
import chardet
import json
import csv

from config import IGNORED_PATTERNS, BLACKLISTED_DOMAINS
from link_utils import (
    normalize_url, is_url_ignored,
    analyze_link, extract_urls_from_text
)

# ======================
# Logging
# ======================

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ======================
# Constants
# ======================

# Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
TEXT_FILE_EXTENSIONS = {
    '.txt', '.log', '.csv', '.json', '.xml', '.yaml', '.yml',
    '.html', '.htm', '.md', '.rst', '.ini', '.cfg', '.conf',
    '.php', '.js', '.py', '.java', '.cpp', '.c', '.h', '.cs',
    '.sql', '.sh', '.bash', '.ps1', '.bat', '.cmd'
}

# Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¶ØºÙˆØ·Ø©
ARCHIVE_FILE_EXTENSIONS = {
    '.zip', '.rar', '.7z', '.tar', '.gz', '.bz2', '.xz',
    '.tgz', '.tbz2', '.txz'
}

# Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØªØ¨ÙŠØ©
OFFICE_FILE_EXTENSIONS = {
    '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.odt', '.ods', '.odp', '.pdf'
}

# Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
OTHER_SUPPORTED_EXTENSIONS = {
    '.sqlite', '.db', '.sqlite3', '.db3'
}

# Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
ALL_SUPPORTED_EXTENSIONS = (
    TEXT_FILE_EXTENSIONS |
    ARCHIVE_FILE_EXTENSIONS |
    OFFICE_FILE_EXTENSIONS |
    OTHER_SUPPORTED_EXTENSIONS
)

# Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„Ù†Øµ
URL_PATTERNS = [
    # Ø±ÙˆØ§Ø¨Ø· ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…
    r't\.me/[A-Za-z0-9_+-]+',
    r'telegram\.me/[A-Za-z0-9_+-]+',
    r'tg://[A-Za-z0-9_?=&-]+',
    
    # Ø±ÙˆØ§Ø¨Ø· ÙˆØ§ØªØ³Ø§Ø¨
    r'chat\.whatsapp\.com/[A-Za-z0-9_-]+',
    r'whatsapp\.com/channel/[A-Za-z0-9_-]+',
    r'wa\.me/[0-9]+',
    
    # Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù…Ø©
    r'https?://[^\s<>"\'{}|\\^`\[\]]+',
    r'www\.[^\s<>"\'{}|\\^`\[\]]+\.[^\s<>"\'{}|\\^`\[\]]+',
]

# ======================
# Text File Processing
# ======================

def detect_file_encoding(filepath: str) -> str:
    """ÙƒØ´Ù ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù"""
    try:
        with open(filepath, 'rb') as f:
            raw_data = f.read(10000)  # Ù‚Ø±Ø§Ø¡Ø© Ø£ÙˆÙ„ 10KB Ù„Ù„ÙƒØ´Ù
            result = chardet.detect(raw_data)
            
            encoding = result.get('encoding', 'utf-8')
            confidence = result.get('confidence', 0)
            
            if confidence < 0.7:
                logger.warning(f"Low encoding confidence for {filepath}: {encoding} ({confidence})")
                return 'utf-8'
            
            # ØªØ­ÙˆÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
            if encoding.lower() in ['windows-1256', 'iso-8859-6', 'arabic']:
                return 'cp1256'
            
            return encoding.lower()
            
    except Exception as e:
        logger.error(f"Error detecting encoding for {filepath}: {e}")
        return 'utf-8'

def read_text_file(filepath: str, encoding: str = 'utf-8') -> str:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ù†ØµÙŠ"""
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø¨Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ø­Ø¯Ø¯
        try:
            with open(filepath, 'r', encoding=encoding, errors='ignore') as f:
                return f.read()
        except UnicodeDecodeError:
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ±Ù…ÙŠØ²Ø§Øª Ø£Ø®Ø±Ù‰ Ø´Ø§Ø¦Ø¹Ø©
            encodings_to_try = ['utf-8-sig', 'cp1256', 'latin-1', 'iso-8859-1']
            
            for enc in encodings_to_try:
                try:
                    with open(filepath, 'r', encoding=enc, errors='ignore') as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            
            # Ø¥Ø°Ø§ ÙØ´Ù„Øª ÙƒÙ„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§ØªØŒ Ø§Ø³ØªØ®Ø¯Ù… binary mode
            with open(filepath, 'rb') as f:
                content = f.read()
                return content.decode('utf-8', errors='ignore')
                
    except Exception as e:
        logger.error(f"Error reading file {filepath}: {e}")
        return ""

def extract_urls_from_text_content(text: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ"""
    if not text:
        return []
    
    urls = []
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… regex Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
    for pattern in URL_PATTERNS:
        matches = re.findall(pattern, text, re.IGNORECASE)
        urls.extend(matches)
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ¥Ø¶Ø§ÙØ© https:// Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    cleaned_urls = []
    for url in urls:
        url = url.strip()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not url.startswith(('http://', 'https://')):
            if url.startswith('www.'):
                url = 'https://' + url
            elif '://' not in url:
                # Ø±ÙˆØ§Ø¨Ø· ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… Ø¨Ø¯ÙˆÙ† Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„
                if url.startswith(('t.me/', 'telegram.me/', 'tg://')):
                    url = 'https://' + url if not url.startswith('tg://') else url
        
        cleaned_urls.append(url)
    
    return list(set(cleaned_urls))  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±

def process_text_file(filepath: str) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù†ØµÙŠ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
    results = {
        'filepath': filepath,
        'filename': os.path.basename(filepath),
        'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0,
        'encoding': 'unknown',
        'total_urls_found': 0,
        'valid_urls': 0,
        'telegram_urls': 0,
        'whatsapp_urls': 0,
        'other_urls': 0,
        'ignored_urls': 0,
        'extracted_urls': [],
        'valid_urls_list': [],
        'error': None
    }
    
    try:
        # ÙƒØ´Ù ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù
        encoding = detect_file_encoding(filepath)
        results['encoding'] = encoding
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù
        content = read_text_file(filepath, encoding)
        
        if not content:
            results['error'] = 'File is empty or cannot be read'
            return results
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        raw_urls = extract_urls_from_text_content(content)
        results['total_urls_found'] = len(raw_urls)
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø±Ø§Ø¨Ø·
        for url in raw_urls:
            try:
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·
                normalized = normalize_url(url)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ¬Ø§Ù‡Ù„
                ignored, ignore_reason = is_url_ignored(normalized)
                if ignored:
                    results['ignored_urls'] += 1
                    continue
                
                results['extracted_urls'].append(normalized)
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·
                analysis = analyze_link(normalized)
                
                if analysis['is_valid']:
                    results['valid_urls'] += 1
                    results['valid_urls_list'].append({
                        'url': normalized,
                        'platform': analysis['platform'],
                        'link_type': analysis['link_type'],
                        'should_collect': analysis['should_collect']
                    })
                    
                    # ØªØ¹Ø¯Ø§Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†ØµØ©
                    if analysis['platform'] == 'telegram':
                        results['telegram_urls'] += 1
                    elif analysis['platform'] == 'whatsapp':
                        results['whatsapp_urls'] += 1
                    else:
                        results['other_urls'] += 1
                
            except Exception as e:
                logger.error(f"Error processing URL {url}: {e}")
                continue
        
        logger.info(f"Processed text file: {filepath} - Found {results['valid_urls']} valid URLs")
        
    except Exception as e:
        logger.error(f"Error processing text file {filepath}: {e}")
        results['error'] = str(e)
    
    return results

# ======================
# Archive File Processing
# ======================

def extract_archive(filepath: str, extract_to: str) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù„Ù Ù…Ø¶ØºÙˆØ·"""
    extracted_files = []
    
    try:
        if not os.path.exists(filepath):
            logger.error(f"Archive file not found: {filepath}")
            return extracted_files
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        os.makedirs(extract_to, exist_ok=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if filepath.endswith('.zip'):
            with zipfile.ZipFile(filepath, 'r') as zip_ref:
                zip_ref.extractall(extract_to)
                extracted_files = zip_ref.namelist()
        
        elif filepath.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2', '.tbz2', '.tar.xz', '.txz')):
            with tarfile.open(filepath, 'r:*') as tar_ref:
                tar_ref.extractall(extract_to)
                extracted_files = tar_ref.getnames()
        
        elif filepath.endswith('.rar'):
            try:
                import rarfile
                with rarfile.RarFile(filepath) as rar_ref:
                    rar_ref.extractall(extract_to)
                    extracted_files = rar_ref.namelist()
            except ImportError:
                logger.error("rarfile library not installed. Cannot extract RAR files.")
                return []
        
        elif filepath.endswith('.7z'):
            try:
                import py7zr
                with py7zr.SevenZipFile(filepath, 'r') as sz_ref:
                    sz_ref.extractall(extract_to)
                    extracted_files = sz_ref.getnames()
            except ImportError:
                logger.error("py7zr library not installed. Cannot extract 7z files.")
                return []
        
        logger.info(f"Extracted {len(extracted_files)} files from {filepath}")
        
    except Exception as e:
        logger.error(f"Error extracting archive {filepath}: {e}")
    
    return extracted_files

def process_archive_file(filepath: str) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù…Ø¶ØºÙˆØ· ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ø­ØªÙˆÙŠØ§ØªÙ‡"""
    results = {
        'filepath': filepath,
        'filename': os.path.basename(filepath),
        'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0,
        'extracted_files': 0,
        'processed_files': 0,
        'total_urls_found': 0,
        'valid_urls': 0,
        'telegram_urls': 0,
        'whatsapp_urls': 0,
        'other_urls': 0,
        'ignored_urls': 0,
        'valid_urls_list': [],
        'extracted_files_info': [],
        'error': None
    }
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù…Ø¤Ù‚Øª Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
        with tempfile.TemporaryDirectory() as temp_dir:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ·
            extracted_files = extract_archive(filepath, temp_dir)
            results['extracted_files'] = len(extracted_files)
            
            if not extracted_files:
                results['error'] = 'No files extracted or archive is empty'
                return results
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„Ù Ù…Ø³ØªØ®Ø±Ø¬
            for extracted_file in extracted_files:
                extracted_path = os.path.join(temp_dir, extracted_file)
                
                # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø³Ø§Ø± Ù…ÙˆØ¬ÙˆØ¯ ÙˆÙ„ÙŠØ³ Ù…Ø¬Ù„Ø¯Ø§Ù‹
                if not os.path.isfile(extracted_path):
                    continue
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
                file_ext = os.path.splitext(extracted_file)[1].lower()
                
                file_info = {
                    'filename': extracted_file,
                    'filepath': extracted_path,
                    'file_size': os.path.getsize(extracted_path),
                    'processed': False,
                    'urls_found': 0,
                    'valid_urls': 0,
                    'error': None
                }
                
                try:
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ© ÙÙ‚Ø·
                    if file_ext in TEXT_FILE_EXTENSIONS:
                        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ
                        text_results = process_text_file(extracted_path)
                        
                        file_info['processed'] = True
                        file_info['urls_found'] = text_results['total_urls_found']
                        file_info['valid_urls'] = text_results['valid_urls']
                        
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
                        results['processed_files'] += 1
                        results['total_urls_found'] += text_results['total_urls_found']
                        results['valid_urls'] += text_results['valid_urls']
                        results['telegram_urls'] += text_results['telegram_urls']
                        results['whatsapp_urls'] += text_results['whatsapp_urls']
                        results['other_urls'] += text_results['other_urls']
                        results['ignored_urls'] += text_results['ignored_urls']
                        
                        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµØ§Ù„Ø­Ø©
                        for url_info in text_results.get('valid_urls_list', []):
                            if url_info['should_collect']:
                                results['valid_urls_list'].append({
                                    'url': url_info['url'],
                                    'source_file': extracted_file,
                                    'platform': url_info['platform'],
                                    'link_type': url_info['link_type']
                                })
                    
                    else:
                        file_info['error'] = f'Unsupported file type: {file_ext}'
                
                except Exception as e:
                    file_info['error'] = str(e)
                    logger.error(f"Error processing extracted file {extracted_file}: {e}")
                
                results['extracted_files_info'].append(file_info)
        
        logger.info(f"Processed archive: {filepath} - Found {results['valid_urls']} valid URLs in {results['processed_files']} files")
        
    except Exception as e:
        logger.error(f"Error processing archive file {filepath}: {e}")
        results['error'] = str(e)
    
    return results

# ======================
# Office File Processing
# ======================

def process_office_file(filepath: str) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù…ÙƒØªØ¨ÙŠ (Word, Excel, PDF, etc.)"""
    results = {
        'filepath': filepath,
        'filename': os.path.basename(filepath),
        'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0,
        'total_urls_found': 0,
        'valid_urls': 0,
        'telegram_urls': 0,
        'whatsapp_urls': 0,
        'other_urls': 0,
        'ignored_urls': 0,
        'valid_urls_list': [],
        'error': None
    }
    
    try:
        content = ""
        file_ext = os.path.splitext(filepath)[1].lower()
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if file_ext == '.pdf':
            try:
                import PyPDF2
                with open(filepath, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        content += page.extract_text() or ""
            except ImportError:
                logger.warning("PyPDF2 not installed. Cannot extract text from PDF.")
                results['error'] = 'PDF extraction requires PyPDF2 library'
                return results
            except Exception as e:
                logger.error(f"Error extracting text from PDF: {e}")
                results['error'] = str(e)
                return results
        
        elif file_ext in ['.docx', '.pptx', '.xlsx']:
            try:
                from docx import Document
                doc = Document(filepath)
                for paragraph in doc.paragraphs:
                    content += paragraph.text + "\n"
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
                for table in doc.tables:
                    for row in table.rows:
                        for cell in row.cells:
                            content += cell.text + " "
            except ImportError:
                logger.warning("python-docx not installed. Cannot extract text from Office files.")
                results['error'] = 'Office file extraction requires python-docx library'
                return results
            except Exception as e:
                logger.error(f"Error extracting text from Office file: {e}")
                results['error'] = str(e)
                return results
        
        elif file_ext in ['.doc', '.ppt', '.xls']:
            # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„
            results['error'] = 'Old Office format (.doc, .ppt, .xls) requires conversion'
            return results
        
        else:
            results['error'] = f'Unsupported Office file type: {file_ext}'
            return results
        
        if not content:
            results['error'] = 'No text content extracted'
            return results
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        urls = extract_urls_from_text_content(content)
        results['total_urls_found'] = len(urls)
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø±Ø§Ø¨Ø·
        for url in urls:
            try:
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·
                normalized = normalize_url(url)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ¬Ø§Ù‡Ù„
                ignored, ignore_reason = is_url_ignored(normalized)
                if ignored:
                    results['ignored_urls'] += 1
                    continue
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·
                analysis = analyze_link(normalized)
                
                if analysis['is_valid'] and analysis['should_collect']:
                    results['valid_urls'] += 1
                    results['valid_urls_list'].append({
                        'url': normalized,
                        'platform': analysis['platform'],
                        'link_type': analysis['link_type']
                    })
                    
                    # ØªØ¹Ø¯Ø§Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†ØµØ©
                    if analysis['platform'] == 'telegram':
                        results['telegram_urls'] += 1
                    elif analysis['platform'] == 'whatsapp':
                        results['whatsapp_urls'] += 1
                    else:
                        results['other_urls'] += 1
                
            except Exception as e:
                logger.error(f"Error processing URL {url}: {e}")
                continue
        
        logger.info(f"Processed office file: {filepath} - Found {results['valid_urls']} valid URLs")
        
    except Exception as e:
        logger.error(f"Error processing office file {filepath}: {e}")
        results['error'] = str(e)
    
    return results

# ======================
# Database File Processing
# ======================

def process_sqlite_file(filepath: str) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù SQLite ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
    results = {
        'filepath': filepath,
        'filename': os.path.basename(filepath),
        'file_size': os.path.getsize(filepath) if os.path.exists(filepath) else 0,
        'tables_found': 0,
        'total_urls_found': 0,
        'valid_urls': 0,
        'telegram_urls': 0,
        'whatsapp_urls': 0,
        'other_urls': 0,
        'ignored_urls': 0,
        'valid_urls_list': [],
        'error': None
    }
    
    try:
        import sqlite3
        
        if not os.path.exists(filepath):
            results['error'] = 'File not found'
            return results
        
        # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = sqlite3.connect(filepath)
        cursor = conn.cursor()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = cursor.fetchall()
        results['tables_found'] = len(tables)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø¬Ø¯ÙˆÙ„ Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        urls_found = set()
        
        for table in tables:
            table_name = table[0]
            
            try:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø¹Ù…Ø¯Ø© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø·
                url_columns = []
                for col in columns:
                    col_name = col[1].lower()
                    if any(keyword in col_name for keyword in ['url', 'link', 'href', 'telegram', 'whatsapp', 't.me']):
                        url_columns.append(col[1])
                
                if url_columns:
                    # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
                    for col in url_columns:
                        cursor.execute(f"SELECT DISTINCT {col} FROM {table_name} WHERE {col} IS NOT NULL AND {col} != ''")
                        rows = cursor.fetchall()
                        
                        for row in rows:
                            value = str(row[0])
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù†Øµ
                            extracted_urls = extract_urls_from_text_content(value)
                            urls_found.update(extracted_urls)
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù†ØµÙŠØ©
                for col in columns:
                    col_name = col[1]
                    col_type = col[2].lower()
                    
                    if 'text' in col_type or 'char' in col_type or 'varchar' in col_type:
                        try:
                            cursor.execute(f"SELECT {col_name} FROM {table_name} WHERE {col_name} LIKE '%t.me%' OR {col_name} LIKE '%telegram.me%' OR {col_name} LIKE '%whatsapp.com%' LIMIT 100")
                            rows = cursor.fetchall()
                            
                            for row in rows:
                                value = str(row[0])
                                extracted_urls = extract_urls_from_text_content(value)
                                urls_found.update(extracted_urls)
                        except:
                            continue
            
            except Exception as e:
                logger.error(f"Error processing table {table_name}: {e}")
                continue
        
        conn.close()
        
        results['total_urls_found'] = len(urls_found)
        
        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ø±Ø§Ø¨Ø·
        for url in urls_found:
            try:
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø±Ø§Ø¨Ø·
                normalized = normalize_url(url)
                
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ¬Ø§Ù‡Ù„
                ignored, ignore_reason = is_url_ignored(normalized)
                if ignored:
                    results['ignored_urls'] += 1
                    continue
                
                # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±Ø§Ø¨Ø·
                analysis = analyze_link(normalized)
                
                if analysis['is_valid'] and analysis['should_collect']:
                    results['valid_urls'] += 1
                    results['valid_urls_list'].append({
                        'url': normalized,
                        'platform': analysis['platform'],
                        'link_type': analysis['link_type']
                    })
                    
                    # ØªØ¹Ø¯Ø§Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†ØµØ©
                    if analysis['platform'] == 'telegram':
                        results['telegram_urls'] += 1
                    elif analysis['platform'] == 'whatsapp':
                        results['whatsapp_urls'] += 1
                    else:
                        results['other_urls'] += 1
                
            except Exception as e:
                logger.error(f"Error processing URL {url}: {e}")
                continue
        
        logger.info(f"Processed SQLite file: {filepath} - Found {results['valid_urls']} valid URLs in {results['tables_found']} tables")
        
    except ImportError:
        results['error'] = 'SQLite processing requires sqlite3 module'
    except Exception as e:
        logger.error(f"Error processing SQLite file {filepath}: {e}")
        results['error'] = str(e)
    
    return results

# ======================
# Generic File Processing
# ======================

def get_file_type(filepath: str) -> str:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù"""
    if not os.path.exists(filepath):
        return 'not_found'
    
    ext = os.path.splitext(filepath)[1].lower()
    
    if ext in TEXT_FILE_EXTENSIONS:
        return 'text'
    elif ext in ARCHIVE_FILE_EXTENSIONS:
        return 'archive'
    elif ext in OFFICE_FILE_EXTENSIONS:
        return 'office'
    elif ext in OTHER_SUPPORTED_EXTENSIONS:
        return 'database'
    else:
        return 'unsupported'

def process_file(filepath: str) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù†Ù‡"""
    file_type = get_file_type(filepath)
    
    if not os.path.exists(filepath):
        return {
            'filepath': filepath,
            'filename': os.path.basename(filepath),
            'error': 'File not found',
            'file_type': 'not_found'
        }
    
    logger.info(f"Processing file: {filepath} (Type: {file_type})")
    
    if file_type == 'text':
        return process_text_file(filepath)
    
    elif file_type == 'archive':
        return process_archive_file(filepath)
    
    elif file_type == 'office':
        return process_office_file(filepath)
    
    elif file_type == 'database':
        return process_sqlite_file(filepath)
    
    else:
        return {
            'filepath': filepath,
            'filename': os.path.basename(filepath),
            'file_size': os.path.getsize(filepath),
            'error': f'Unsupported file type: {os.path.splitext(filepath)[1]}',
            'file_type': 'unsupported'
        }

def process_directory(directory_path: str, recursive: bool = True) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª"""
    results = {
        'directory': directory_path,
        'total_files': 0,
        'processed_files': 0,
        'unsupported_files': 0,
        'failed_files': 0,
        'total_urls_found': 0,
        'valid_urls': 0,
        'telegram_urls': 0,
        'whatsapp_urls': 0,
        'other_urls': 0,
        'files_processed': [],
        'valid_urls_list': [],
        'summary_by_type': {
            'text': 0,
            'archive': 0,
            'office': 0,
            'database': 0,
            'unsupported': 0
        }
    }
    
    try:
        if not os.path.exists(directory_path) or not os.path.isdir(directory_path):
            results['error'] = 'Directory not found or not a directory'
            return results
        
        # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        all_files = []
        
        if recursive:
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    all_files.append(os.path.join(root, file))
        else:
            for item in os.listdir(directory_path):
                item_path = os.path.join(directory_path, item)
                if os.path.isfile(item_path):
                    all_files.append(item_path)
        
        results['total_files'] = len(all_files)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ù„Ù
        for filepath in all_files:
            try:
                file_type = get_file_type(filepath)
                results['summary_by_type'][file_type] += 1
                
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
                if file_type == 'unsupported':
                    results['unsupported_files'] += 1
                    continue
                
                # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
                file_results = process_file(filepath)
                
                file_summary = {
                    'filepath': filepath,
                    'filename': os.path.basename(filepath),
                    'file_type': file_type,
                    'file_size': file_results.get('file_size', 0),
                    'urls_found': file_results.get('total_urls_found', 0),
                    'valid_urls': file_results.get('valid_urls', 0),
                    'error': file_results.get('error')
                }
                
                results['files_processed'].append(file_summary)
                results['processed_files'] += 1
                
                if not file_results.get('error'):
                    results['total_urls_found'] += file_results.get('total_urls_found', 0)
                    results['valid_urls'] += file_results.get('valid_urls', 0)
                    results['telegram_urls'] += file_results.get('telegram_urls', 0)
                    results['whatsapp_urls'] += file_results.get('whatsapp_urls', 0)
                    results['other_urls'] += file_results.get('other_urls', 0)
                    
                    # Ø¬Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµØ§Ù„Ø­Ø©
                    for url_info in file_results.get('valid_urls_list', []):
                        results['valid_urls_list'].append({
                            'url': url_info['url'],
                            'source_file': os.path.basename(filepath),
                            'platform': url_info.get('platform'),
                            'link_type': url_info.get('link_type')
                        })
                else:
                    results['failed_files'] += 1
                
                logger.info(f"Processed: {filepath} - {file_results.get('valid_urls', 0)} valid URLs")
                
            except Exception as e:
                logger.error(f"Error processing file {filepath}: {e}")
                results['failed_files'] += 1
                continue
        
        logger.info(f"Directory processing completed: {results['valid_urls']} valid URLs found in {results['processed_files']} files")
        
    except Exception as e:
        logger.error(f"Error processing directory {directory_path}: {e}")
        results['error'] = str(e)
    
    return results

# ======================
# Export Functions
# ======================

def export_extracted_urls(results: Dict, output_file: str = None) -> str:
    """ØªØµØ¯ÙŠØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù"""
    try:
        if not results.get('valid_urls_list'):
            logger.warning("No valid URLs to export")
            return ""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ±Ù‡
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"extracted_urls_{timestamp}.txt"
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†ØµØ© ÙˆØ§Ù„Ù†ÙˆØ¹
        telegram_groups = []
        telegram_channels = []
        whatsapp_groups = []
        other_urls = []
        
        for url_info in results['valid_urls_list']:
            url = url_info['url']
            platform = url_info.get('platform', '')
            link_type = url_info.get('link_type', '')
            
            if platform == 'telegram':
                if link_type in ['public_group', 'private_group']:
                    telegram_groups.append(url)
                elif link_type == 'channel':
                    telegram_channels.append(url)
                else:
                    other_urls.append(url)
            elif platform == 'whatsapp' and link_type == 'group':
                whatsapp_groups.append(url)
            else:
                other_urls.append(url)
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(f"# Extracted URLs Report\n")
            f.write(f"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Total Valid URLs: {len(results['valid_urls_list'])}\n")
            f.write("#" * 60 + "\n\n")
            
            if telegram_groups:
                f.write(f"# Telegram Groups ({len(telegram_groups)})\n")
                f.write("#" * 40 + "\n")
                for url in telegram_groups:
                    f.write(f"{url}\n")
                f.write("\n")
            
            if telegram_channels:
                f.write(f"# Telegram Channels ({len(telegram_channels)})\n")
                f.write("#" * 40 + "\n")
                for url in telegram_channels:
                    f.write(f"{url}\n")
                f.write("\n")
            
            if whatsapp_groups:
                f.write(f"# WhatsApp Groups ({len(whatsapp_groups)})\n")
                f.write("#" * 40 + "\n")
                for url in whatsapp_groups:
                    f.write(f"{url}\n")
                f.write("\n")
            
            if other_urls:
                f.write(f"# Other URLs ({len(other_urls)})\n")
                f.write("#" * 40 + "\n")
                for url in other_urls:
                    f.write(f"{url}\n")
        
        logger.info(f"Exported {len(results['valid_urls_list'])} URLs to {output_file}")
        return output_file
        
    except Exception as e:
        logger.error(f"Error exporting URLs: {e}")
        return ""

def save_processing_report(results: Dict, report_file: str = None) -> str:
    """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù JSON"""
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙˆÙÙŠØ±Ù‡
        if not report_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = f"processing_report_{timestamp}.json"
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØµØ¯ÙŠØ±
        export_data = {
            'report_date': datetime.now().isoformat(),
            'summary': {
                'total_files': results.get('total_files', 0),
                'processed_files': results.get('processed_files', 0),
                'unsupported_files': results.get('unsupported_files', 0),
                'failed_files': results.get('failed_files', 0),
                'total_urls_found': results.get('total_urls_found', 0),
                'valid_urls': results.get('valid_urls', 0),
                'telegram_urls': results.get('telegram_urls', 0),
                'whatsapp_urls': results.get('whatsapp_urls', 0),
                'other_urls': results.get('other_urls', 0)
            },
            'file_types_summary': results.get('summary_by_type', {}),
            'files_processed': results.get('files_processed', []),
            'valid_urls_count': len(results.get('valid_urls_list', [])),
            'error': results.get('error')
        }
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù„Ù
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved processing report to {report_file}")
        return report_file
        
    except Exception as e:
        logger.error(f"Error saving processing report: {e}")
        return ""

# ======================
# Test Functions
# ======================

def test_file_extraction():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª"""
    print("ğŸ”§ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª...")
    print("=" * 80)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†ØµÙŠ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_content = """
    Telegram groups:
    https://t.me/test_group_1
    https://t.me/+invite123
    t.me/test_group_2
    
    WhatsApp groups:
    https://chat.whatsapp.com/group1
    https://whatsapp.com/channel/channel1
    
    Other links:
    https://t.me/channel_news (channel)
    https://facebook.com/groups/test
    https://discord.gg/test
    
    Invalid/ignored:
    https://t.me/c/1234567890 (private channel)
    https://t.me/botfather (bot)
    """
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        f.write(test_content)
        test_file = f.name
    
    try:
        print(f"\nğŸ“„ Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù Ù†ØµÙŠ: {test_file}")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
        results = process_text_file(test_file)
        
        print(f"\nğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
        print(f"   â€¢ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: {results['filename']}")
        print(f"   â€¢ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: {results['file_size']} bytes")
        print(f"   â€¢ Ø§Ù„ØªØ±Ù…ÙŠØ²: {results['encoding']}")
        print(f"   â€¢ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©: {results['total_urls_found']}")
        print(f"   â€¢ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµØ§Ù„Ø­Ø©: {results['valid_urls']}")
        print(f"   â€¢ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {results['telegram_urls']}")
        print(f"   â€¢ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª ÙˆØ§ØªØ³Ø§Ø¨: {results['whatsapp_urls']}")
        print(f"   â€¢ Ø±ÙˆØ§Ø¨Ø· Ø£Ø®Ø±Ù‰: {results['other_urls']}")
        print(f"   â€¢ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ØªØ¬Ø§Ù‡Ù„Ø©: {results['ignored_urls']}")
        
        if results.get('error'):
            print(f"   â€¢ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {results['error']}")
        
        print(f"\nğŸ”— Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµØ§Ù„Ø­Ø©:")
        for url_info in results.get('valid_urls_list', []):
            print(f"   â€¢ {url_info['url']} ({url_info['platform']}/{url_info['link_type']})")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
        print(f"\nğŸ’¾ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±...")
        export_file = export_extracted_urls(results, 'test_export.txt')
        if export_file and os.path.exists(export_file):
            print(f"   âœ… ØªÙ… Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰: {export_file}")
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø± Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            with open(export_file, 'r', encoding='utf-8') as f:
                print(f"\nğŸ“„ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ØµØ¯Ø±:")
                print("-" * 40)
                print(f.read()[:500])  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 500 Ø­Ø±Ù ÙÙ‚Ø·
                print("-" * 40)
            
            # ØªÙ†Ø¸ÙŠÙ
            os.remove(export_file)
        
        print("\nâœ… Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­!")
        
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        if os.path.exists(test_file):
            os.remove(test_file)

# ======================
# Main Entry Point
# ======================

if __name__ == "__main__":
    import sys
    
    print("ğŸš€ ØªØ´ØºÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª...")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    test_file_extraction()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©:")
    
    print("\nğŸ“„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©:")
    for ext in sorted(TEXT_FILE_EXTENSIONS):
        print(f"   â€¢ {ext}")
    
    print("\nğŸ“¦ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¶ØºÙˆØ·Ø©:")
    for ext in sorted(ARCHIVE_FILE_EXTENSIONS):
        print(f"   â€¢ {ext}")
    
    print("\nğŸ“Š Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØªØ¨ÙŠØ©:")
    for ext in sorted(OFFICE_FILE_EXTENSIONS):
        print(f"   â€¢ {ext}")
    
    print("\nğŸ—„ï¸ Ù…Ù„ÙØ§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    for ext in sorted(OTHER_SUPPORTED_EXTENSIONS):
        print(f"   â€¢ {ext}")
    
    print(f"\nğŸ¯ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: {len(ALL_SUPPORTED_EXTENSIONS)} Ù†ÙˆØ¹")
    
    print("\nâœ… Ø£Ø¯ÙˆØ§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø¹Ù…Ù„!")
