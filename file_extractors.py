import os
import tempfile
import logging
import asyncio
from typing import List, Set, Dict, Optional
from pathlib import Path

from telethon import TelegramClient
from telethon.tl.types import Message

from config import EXPORT_DIR
from link_utils import URL_REGEX, clean_link, is_allowed_link

# ======================
# Logging Configuration
# ======================

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ======================
# Constants
# ======================

# Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© ÙˆØ§Ù…ØªØ¯Ø§Ø¯Ø§ØªÙ‡Ø§
SUPPORTED_EXTENSIONS = {
    '.pdf': 'application/pdf',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.txt': 'text/plain',
    '.rtf': 'application/rtf',
    '.odt': 'application/vnd.oasis.opendocument.text',
    '.csv': 'text/csv',
    '.html': 'text/html',
    '.htm': 'text/html',
    '.xml': 'text/xml',
    '.json': 'application/json',
}

# Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù (50 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª)
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

# Ø­Ø¬Ù… Ø§Ù„ÙƒØªÙ„Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©
CHUNK_SIZE = 1024 * 1024  # 1MB

# ======================
# Helper Functions
# ======================

def is_file_supported(filename: str, mime_type: str = None) -> bool:
    """
    Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§
    
    Args:
        filename: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        mime_type: Ù†ÙˆØ¹ MIME (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        
    Returns:
        bool: True Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§
    """
    if not filename:
        return False
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
    file_ext = os.path.splitext(filename.lower())[1]
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯
    if file_ext in SUPPORTED_EXTENSIONS:
        return True
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†ÙˆØ¹ MIME
    if mime_type and mime_type in SUPPORTED_EXTENSIONS.values():
        return True
    
    return False

def get_file_extension(filename: str) -> str:
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
    
    Args:
        filename: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù
        
    Returns:
        str: Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„Ù…Ù„Ù
    """
    return os.path.splitext(filename.lower())[1]

# ======================
# Main Extraction Function
# ======================

async def extract_links_from_file(
    client: TelegramClient,
    message: Message
) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„ÙØ§Øª Ù…ØªÙ†ÙˆØ¹Ø©
    
    Args:
        client: Ø¹Ù…ÙŠÙ„ Telethon
        message: Ø±Ø³Ø§Ù„Ø© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„Ù
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    if not message or not message.file:
        logger.debug("No file in message")
        return []
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
    file_size = message.file.size or 0
    if file_size > MAX_FILE_SIZE:
        logger.warning(f"File too large: {file_size} bytes (max: {MAX_FILE_SIZE})")
        return []
    
    filename = message.file.name or "unknown_file"
    mime_type = message.file.mime_type or ""
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§
    if not is_file_supported(filename, mime_type):
        logger.debug(f"Unsupported file type: {filename} ({mime_type})")
        return []
    
    links: Set[str] = set()
    
    try:
        logger.info(f"Processing file: {filename} ({file_size} bytes)")
        
        with tempfile.TemporaryDirectory() as tmpdir:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
            temp_path = os.path.join(tmpdir, filename)
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
            logger.debug(f"Downloading file to: {temp_path}")
            await client.download_media(message, temp_path)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
            file_ext = get_file_extension(filename)
            
            if file_ext == '.pdf' or mime_type == 'application/pdf':
                file_links = await extract_from_pdf_async(temp_path)
            elif file_ext == '.docx' or 'wordprocessingml.document' in mime_type:
                file_links = await extract_from_docx_async(temp_path)
            elif file_ext == '.txt' or mime_type == 'text/plain':
                file_links = await extract_from_txt_async(temp_path)
            elif file_ext == '.rtf' or mime_type == 'application/rtf':
                file_links = await extract_from_rtf_async(temp_path)
            elif file_ext == '.odt' or 'opendocument.text' in mime_type:
                file_links = await extract_from_odt_async(temp_path)
            elif file_ext in ['.html', '.htm'] or 'text/html' in mime_type:
                file_links = await extract_from_html_async(temp_path)
            elif file_ext == '.xml' or 'text/xml' in mime_type:
                file_links = await extract_from_xml_async(temp_path)
            elif file_ext == '.json' or 'application/json' in mime_type:
                file_links = await extract_from_json_async(temp_path)
            elif file_ext == '.csv' or 'text/csv' in mime_type:
                file_links = await extract_from_csv_async(temp_path)
            else:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ…Ù„Ù Ù†ØµÙŠ Ø¹Ø§Ù…
                file_links = await extract_generic_text_async(temp_path)
            
            # ØªÙ†Ø¸ÙŠÙ ÙˆÙÙ„ØªØ±Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            for link in file_links:
                cleaned = clean_link(link)
                if cleaned and is_allowed_link(cleaned):
                    links.add(cleaned)
            
            logger.info(f"Extracted {len(links)} links from file: {filename}")
            
            return list(links)
            
    except Exception as e:
        logger.error(f"Error extracting links from file {filename}: {e}")
        return []

# ======================
# PDF Extraction
# ======================

async def extract_from_pdf_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù PDF Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù PDF
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        # ØªØ´ØºÙŠÙ„ ÙÙŠ thread Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± event loop
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_pdf_sync, path
        )
    except Exception as e:
        logger.error(f"PDF extraction error: {e}")
        return []

def extract_from_pdf_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù PDF
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… PyPDF2 Ø£ÙˆÙ„Ø§Ù‹
        try:
            from PyPDF2 import PdfReader
            
            reader = PdfReader(path)
            logger.debug(f"PDF has {len(reader.pages)} pages")
            
            for page_num, page in enumerate(reader.pages, 1):
                try:
                    text = page.extract_text() or ""
                    if text:
                        page_links = URL_REGEX.findall(text)
                        links.update(page_links)
                        logger.debug(f"Page {page_num}: Found {len(page_links)} links")
                except Exception as e:
                    logger.warning(f"Error extracting text from PDF page {page_num}: {e}")
                    continue
            
            if links:
                logger.info(f"PyPDF2 extracted {len(links)} links from PDF")
                return list(links)
            
        except ImportError:
            logger.warning("PyPDF2 is not installed")
        except Exception as e:
            logger.warning(f"PyPDF2 failed: {e}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… pdfplumber ÙƒØ¨Ø¯ÙŠÙ„
        try:
            import pdfplumber
            
            with pdfplumber.open(path) as pdf:
                logger.debug(f"pdfplumber opened PDF with {len(pdf.pages)} pages")
                
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        text = page.extract_text() or ""
                        if text:
                            page_links = URL_REGEX.findall(text)
                            links.update(page_links)
                    except Exception as e:
                        logger.warning(f"Error extracting text with pdfplumber page {page_num}: {e}")
                        continue
                
                if links:
                    logger.info(f"pdfplumber extracted {len(links)} links from PDF")
                    return list(links)
                
        except ImportError:
            logger.warning("pdfplumber is not installed")
        except Exception as e:
            logger.warning(f"pdfplumber failed: {e}")
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙƒÙ…Ù„Ù Ù†ØµÙŠ Ø«Ù†Ø§Ø¦ÙŠ
        try:
            with open(path, 'rb') as f:
                content = f.read()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· URLs ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©
                import re
                url_pattern = rb'https?://[^\x00-\x1F\x7F-\xFF<>"\s]+'
                binary_matches = re.findall(url_pattern, content)
                
                for match in binary_matches:
                    try:
                        url = match.decode('utf-8', errors='ignore')
                        links.add(url)
                    except:
                        pass
                
                if binary_matches:
                    logger.info(f"Binary search found {len(binary_matches)} URL patterns")
                    
        except Exception as e:
            logger.warning(f"Binary extraction failed: {e}")
    
    except Exception as e:
        logger.error(f"PDF extraction failed: {e}")
    
    return list(links)

# ======================
# DOCX Extraction
# ======================

async def extract_from_docx_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù DOCX Ø¨Ø´ÙƒÙ„ ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù DOCX
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_docx_sync, path
        )
    except Exception as e:
        logger.error(f"DOCX extraction error: {e}")
        return []

def extract_from_docx_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù DOCX (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù DOCX
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        from docx import Document
        
        doc = Document(path)
        logger.debug(f"DOCX document opened")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø§Øª
        for para_num, para in enumerate(doc.paragraphs, 1):
            if para.text:
                para_links = URL_REGEX.findall(para.text)
                links.update(para_links)
                if para_links:
                    logger.debug(f"Paragraph {para_num}: Found {len(para_links)} links")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        for table_num, table in enumerate(doc.tables, 1):
            for row_num, row in enumerate(table.rows, 1):
                for cell_num, cell in enumerate(row.cells, 1):
                    if cell.text:
                        cell_links = URL_REGEX.findall(cell.text)
                        links.update(cell_links)
                        if cell_links:
                            logger.debug(f"Table {table_num}, Row {row_num}, Cell {cell_num}: Found {len(cell_links)} links")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„Ø±Ø¤ÙˆØ³ ÙˆØ§Ù„ØªØ°ÙŠÙŠÙ„Ø§Øª
        for section in doc.sections:
            # Ø§Ù„Ø±Ø£Ø³
            header = section.header
            if header:
                for para in header.paragraphs:
                    if para.text:
                        links.update(URL_REGEX.findall(para.text))
            
            # Ø§Ù„ØªØ°ÙŠÙŠÙ„
            footer = section.footer
            if footer:
                for para in footer.paragraphs:
                    if para.text:
                        links.update(URL_REGEX.findall(para.text))
        
        logger.info(f"Extracted {len(links)} links from DOCX")
        return list(links)
    
    except ImportError:
        logger.warning("python-docx is not installed")
        return []
    except Exception as e:
        logger.error(f"DOCX extraction failed: {e}")
        return []

# ======================
# Text File Extraction
# ======================

async def extract_from_txt_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù Ù†ØµÙŠ
    
    Args:
        path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_txt_sync, path
        )
    except Exception as e:
        logger.error(f"TXT extraction error: {e}")
        return []

def extract_from_txt_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù Ù†ØµÙŠ (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© ÙØªØ­ Ø§Ù„Ù…Ù„Ù Ø¨ØªØ´ÙÙŠØ±Ø§Øª Ù…Ø®ØªÙ„ÙØ©
        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1256', 'windows-1256', 'ascii']
        
        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding) as f:
                    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø´ÙƒÙ„ Ù…ØªÙ‚Ø·Ø¹ Ù„Ù„ÙƒÙØ§Ø¡Ø©
                    while True:
                        chunk = f.read(CHUNK_SIZE)
                        if not chunk:
                            break
                        
                        chunk_links = URL_REGEX.findall(chunk)
                        links.update(chunk_links)
                
                logger.info(f"Successfully read TXT with {encoding} encoding")
                break  # Ù†Ø¬Ø­ØŒ ØªÙˆÙ‚Ù Ø¹Ù† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©
                
            except UnicodeDecodeError:
                logger.debug(f"Failed with encoding {encoding}")
                continue
            except Exception as e:
                logger.warning(f"Error reading with encoding {encoding}: {e}")
                continue
        
        logger.info(f"Extracted {len(links)} links from TXT")
        return list(links)
    
    except Exception as e:
        logger.error(f"TXT extraction failed: {e}")
        return []

# ======================
# RTF Extraction
# ======================

async def extract_from_rtf_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù RTF
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù RTF
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_rtf_sync, path
        )
    except Exception as e:
        logger.error(f"RTF extraction error: {e}")
        return []

def extract_from_rtf_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù RTF (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù RTF
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù… striprtf
        try:
            from striprtf.striprtf import rtf_to_text
            
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                rtf_content = f.read()
                text_content = rtf_to_text(rtf_content)
                links.update(URL_REGEX.findall(text_content))
            
            logger.info(f"striprtf extracted {len(links)} links from RTF")
            return list(links)
        
        except ImportError:
            logger.warning("striprtf is not installed, trying basic extraction")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø³ÙŠØ· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Øµ Ø¨ÙŠÙ† Ø£Ù‚ÙˆØ§Ø³ ÙÙŠ RTF
                import re
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Øµ ØºÙŠØ± ØªÙ†Ø³ÙŠÙ‚ÙŠ
                text_matches = re.findall(r'\\\'(..)|(\\u\d+)|([a-zA-Z0-9\s,.!?:/=+_-]+)', content)
                
                extracted_text = ' '.join([''.join(match) for match in text_matches])
                links.update(URL_REGEX.findall(extracted_text))
                
                # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† URLs
                links.update(URL_REGEX.findall(content))
            
            logger.info(f"Basic extraction found {len(links)} links in RTF")
            return list(links)
    
    except Exception as e:
        logger.error(f"RTF extraction failed: {e}")
        return []

# ======================
# ODT Extraction
# ======================

async def extract_from_odt_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù ODT
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù ODT
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_odt_sync, path
        )
    except Exception as e:
        logger.error(f"ODT extraction error: {e}")
        return []

def extract_from_odt_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù ODT (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù ODT
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        # ODT Ù‡Ùˆ Ù…Ù„Ù ZIP ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ XML
        import zipfile
        from xml.etree import ElementTree as ET
        
        with zipfile.ZipFile(path, 'r') as odt_file:
            # Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
            content_xml = odt_file.read('content.xml')
            
            # ØªØ­Ù„ÙŠÙ„ XML
            namespaces = {
                'text': 'urn:oasis:names:tc:opendocument:xmlns:text:1.0',
                'office': 'urn:oasis:names:tc:opendocument:xmlns:office:1.0'
            }
            
            root = ET.fromstring(content_xml)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù†Øµ
            for elem in root.findall('.//text:p', namespaces):
                if elem.text:
                    links.update(URL_REGEX.findall(elem.text))
            
            for elem in root.findall('.//text:span', namespaces):
                if elem.text:
                    links.update(URL_REGEX.findall(elem.text))
            
            logger.info(f"Extracted {len(links)} links from ODT")
            return list(links)
    
    except Exception as e:
        logger.error(f"ODT extraction failed: {e}")
        return []

# ======================
# HTML Extraction
# ======================

async def extract_from_html_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù HTML
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù HTML
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_html_sync, path
        )
    except Exception as e:
        logger.error(f"HTML extraction error: {e}")
        return []

def extract_from_html_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù HTML (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù HTML
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ HTML
            import re
            
            # Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø³Ù…Ø© href
            href_pattern = r'href=[\'"]?([^\'" >]+)[\'"]?'
            href_matches = re.findall(href_pattern, content, re.IGNORECASE)
            links.update(href_matches)
            
            # Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø³Ù…Ø© src
            src_pattern = r'src=[\'"]?([^\'" >]+)[\'"]?'
            src_matches = re.findall(src_pattern, content, re.IGNORECASE)
            links.update(src_matches)
            
            # Ø±ÙˆØ§Ø¨Ø· Ù†ØµÙŠØ© Ø¹Ø§Ø¯ÙŠØ©
            links.update(URL_REGEX.findall(content))
        
        logger.info(f"Extracted {len(links)} links from HTML")
        return list(links)
    
    except Exception as e:
        logger.error(f"HTML extraction failed: {e}")
        return []

# ======================
# XML Extraction
# ======================

async def extract_from_xml_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù XML
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù XML
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_xml_sync, path
        )
    except Exception as e:
        logger.error(f"XML extraction error: {e}")
        return []

def extract_from_xml_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù XML (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù XML
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        import xml.etree.ElementTree as ET
        
        tree = ET.parse(path)
        root = tree.getroot()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù†Ø§ØµØ±
        def extract_text(element):
            text_parts = []
            
            if element.text:
                text_parts.append(element.text)
            
            for child in element:
                text_parts.extend(extract_text(child))
            
            if element.tail:
                text_parts.append(element.tail)
            
            return text_parts
        
        all_text_parts = extract_text(root)
        full_text = ' '.join(all_text_parts)
        
        links.update(URL_REGEX.findall(full_text))
        
        logger.info(f"Extracted {len(links)} links from XML")
        return list(links)
    
    except Exception as e:
        logger.error(f"XML extraction failed: {e}")
        return []

# ======================
# JSON Extraction
# ======================

async def extract_from_json_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù JSON
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_json_sync, path
        )
    except Exception as e:
        logger.error(f"JSON extraction error: {e}")
        return []

def extract_from_json_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù JSON (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù JSON
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        import json
        
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ØªØ­ÙˆÙŠÙ„ JSON Ø¥Ù„Ù‰ Ù†Øµ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        json_text = json.dumps(data)
        links.update(URL_REGEX.findall(json_text))
        
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¹Ù…Ù‚ ÙÙŠ Ø§Ù„Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©
        def find_urls_in_structure(obj):
            if isinstance(obj, str):
                links.update(URL_REGEX.findall(obj))
            elif isinstance(obj, dict):
                for value in obj.values():
                    find_urls_in_structure(value)
            elif isinstance(obj, list):
                for item in obj:
                    find_urls_in_structure(item)
        
        find_urls_in_structure(data)
        
        logger.info(f"Extracted {len(links)} links from JSON")
        return list(links)
    
    except Exception as e:
        logger.error(f"JSON extraction failed: {e}")
        return []

# ======================
# CSV Extraction
# ======================

async def extract_from_csv_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ù„Ù CSV
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù CSV
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_from_csv_sync, path
        )
    except Exception as e:
        logger.error(f"CSV extraction error: {e}")
        return []

def extract_from_csv_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù CSV (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ù…Ù„Ù CSV
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        import csv
        
        with open(path, 'r', encoding='utf-8', errors='ignore') as f:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø¨ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©
            for delimiter in [',', ';', '\t', '|']:
                try:
                    reader = csv.reader(f, delimiter=delimiter)
                    
                    for row in reader:
                        for cell in row:
                            if cell:
                                links.update(URL_REGEX.findall(cell))
                    
                    # Ø¥Ø°Ø§ ÙˆØµÙ„Ù†Ø§ Ø¥Ù„Ù‰ Ù‡Ù†Ø§ØŒ ÙƒØ§Ù† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ ØµØ­ÙŠØ­Ù‹Ø§
                    logger.info(f"CSV read successfully with delimiter '{delimiter}'")
                    break
                    
                except:
                    # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ù…Ø¤Ø´Ø± Ù„Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ù…Ù„Ù
                    f.seek(0)
                    continue
        
        # Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§ØªØŒ Ù‚Ø±Ø§Ø¡Ø© ÙƒÙ…Ù„Ù Ù†ØµÙŠ
        if not links:
            f.seek(0)
            content = f.read()
            links.update(URL_REGEX.findall(content))
        
        logger.info(f"Extracted {len(links)} links from CSV")
        return list(links)
    
    except Exception as e:
        logger.error(f"CSV extraction failed: {e}")
        return []

# ======================
# Generic Text Extraction
# ======================

async def extract_generic_text_async(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø¹Ø§Ù… Ù…Ù† Ø£ÙŠ Ù…Ù„Ù
    
    Args:
        path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    try:
        return await asyncio.get_event_loop().run_in_executor(
            None, extract_generic_text_sync, path
        )
    except Exception as e:
        logger.error(f"Generic text extraction error: {e}")
        return []

def extract_generic_text_sync(path: str) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø¹Ø§Ù… (Ù…ØªØ²Ø§Ù…Ù†)
    
    Args:
        path: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù
        
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
    """
    links: Set[str] = set()
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù ÙƒÙ†Øµ Ø«Ù†Ø§Ø¦ÙŠ
        with open(path, 'rb') as f:
            content = f.read()
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙƒ Ø§Ù„ØªØ´ÙÙŠØ± ÙƒÙ†Øµ
            try:
                text = content.decode('utf-8', errors='ignore')
                links.update(URL_REGEX.findall(text))
            except:
                # Ø§Ù„Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ù† Ø£Ù†Ù…Ø§Ø· URLs ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ©
                import re
                url_pattern = rb'https?://[^\x00-\x1F\x7F-\xFF<>"\s]+'
                binary_matches = re.findall(url_pattern, content)
                
                for match in binary_matches:
                    try:
                        url = match.decode('utf-8', errors='ignore')
                        links.add(url)
                    except:
                        pass
        
        logger.info(f"Generic extraction found {len(links)} links")
        return list(links)
    
    except Exception as e:
        logger.error(f"Generic text extraction failed: {e}")
        return []

# ======================
# Batch Processing
# ======================

async def extract_links_from_files_batch(
    client: TelegramClient,
    messages: List[Message]
) -> Dict[str, List[str]]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª
    
    Args:
        client: Ø¹Ù…ÙŠÙ„ Telethon
        messages: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª
        
    Returns:
        dict: Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
    """
    results = {}
    
    for message in messages:
        try:
            filename = message.file.name or "unknown_file"
            links = await extract_links_from_file(client, message)
            
            if links:
                results[filename] = links
                logger.info(f"Extracted {len(links)} links from {filename}")
            
        except Exception as e:
            logger.error(f"Error processing file in batch: {e}")
    
    return results

# ======================
# Export Functions
# ======================

def save_extracted_links(filename: str, links: List[str]) -> str:
    """
    Ø­ÙØ¸ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù
    
    Args:
        filename: Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ
        links: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        
    Returns:
        str: Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸
    """
    try:
        if not links:
            return ""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù„Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ÙÙˆØ¸
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(os.path.basename(filename))[0]
        export_filename = f"{base_name}_links_{timestamp}.txt"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„ØªØµØ¯ÙŠØ±
        export_path = os.path.join(EXPORT_DIR, "file_extractions")
        os.makedirs(export_path, exist_ok=True)
        
        # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ÙƒØ§Ù…Ù„
        full_path = os.path.join(export_path, export_filename)
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù
        with open(full_path, 'w', encoding='utf-8') as f:
            for link in links:
                f.write(link + "\n")
        
        logger.info(f"Saved {len(links)} links to {full_path}")
        return full_path
    
    except Exception as e:
        logger.error(f"Error saving extracted links: {e}")
        return ""

# ======================
# Test Functions
# ======================

async def test_file_extractors():
    """
    Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ ÙˆØ¸Ø§Ø¦Ù Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª
    """
    print("\n" + "="*50)
    print("ğŸ§ª Testing File Extractors Module")
    print("="*50)
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¯Ø¹Ù… Ø§Ù„Ù…Ù„ÙØ§Øª
    print("\n1. Testing file support:")
    test_files = [
        "document.pdf",
        "data.docx",
        "notes.txt",
        "file.rtf",
        "document.odt",
        "page.html",
        "data.xml",
        "config.json",
        "data.csv",
        "unknown.xyz"
    ]
    
    for filename in test_files:
        supported = is_file_supported(filename)
        status = "âœ…" if supported else "âŒ"
        print(f"   {status} {filename}: {supported}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ©
    print("\n2. Testing text extraction:")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†ØµÙŠ Ø§Ø®ØªØ¨Ø§Ø±ÙŠ
    test_content = """
    Here are some test links:
    Telegram: https://t.me/test_channel
    WhatsApp: https://chat.whatsapp.com/abc123
    Another: https://t.me/joinchat/def456
    """
    
    import tempfile
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
        f.write(test_content)
        temp_file = f.name
    
    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ù…Ù„Ù Ù†ØµÙŠ
        links = await extract_from_txt_async(temp_file)
        print(f"   ğŸ“„ TXT extraction: Found {len(links)} links")
        for link in links:
            print(f"      â€¢ {link}")
    
    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        if os.path.exists(temp_file):
            os.unlink(temp_file)
    
    print("\n" + "="*50)
    print("âœ… File Extractors test completed successfully!")
    print("="*50)

# ======================
# Main Test
# ======================

if __name__ == "__main__":
    import asyncio
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    asyncio.run(test_file_extractors())
